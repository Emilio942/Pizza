#!/usr/bin/env python3
"""
SPATIAL-4.2: Enhanced Spatial-MLLM Deployment Tests

Comprehensive test suite for Spatial-MLLM deployment pipeline validation.
Includes automated tests for spatial features, model versioning, and environment compatibility.
"""

import os
import sys
import json
import time
import torch
import logging
import subprocess
import traceback
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Tuple, Optional, Any
import argparse

# Set up logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class EnhancedSpatialDeploymentTests:
    """Enhanced spatial tests for comprehensive deployment validation."""
    
    def __init__(self, project_root: str = None, docker_mode: bool = False):
        """Initialize the enhanced test suite."""
        self.project_root = Path(project_root) if project_root else Path(__file__).parent.parent.parent
        self.docker_mode = docker_mode
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.results = {}
        self.start_time = time.time()
        
        # Setup paths
        sys.path.insert(0, str(self.project_root))
        
        # Test configuration
        self.test_config = {
            'timeout': 300,  # 5 minutes per test
            'retry_count': 3,
            'min_accuracy_threshold': 0.7,
            'max_memory_usage_gb': 8.0,
            'max_inference_time_ms': 2000
        }
        
        logger.info("üß™ Enhanced Spatial Deployment Tests initialized")
        logger.info(f"Device: {self.device}")
        logger.info(f"Docker mode: {self.docker_mode}")
        logger.info(f"Project root: {self.project_root}")
    
    def validate_environment(self) -> Dict[str, bool]:
        """Validate the deployment environment for Spatial-MLLM."""
        logger.info("üîç Validating deployment environment...")
        validation_results = {}
        
        try:
            # Check Python version
            python_version = sys.version_info
            validation_results['python_version'] = python_version.major == 3 and python_version.minor >= 9
            logger.info(f"Python version: {python_version.major}.{python_version.minor}")
            
            # Check CUDA availability
            cuda_available = torch.cuda.is_available()
            validation_results['cuda_available'] = cuda_available
            
            if cuda_available:
                cuda_version = torch.version.cuda
                device_count = torch.cuda.device_count()
                memory_gb = torch.cuda.get_device_properties(0).total_memory / (1024**3)
                
                validation_results['cuda_memory_sufficient'] = memory_gb >= 6.0
                validation_results['cuda_device_count'] = device_count >= 1
                
                logger.info(f"CUDA version: {cuda_version}")
                logger.info(f"GPU memory: {memory_gb:.1f} GB")
                logger.info(f"Device count: {device_count}")
            
            # Check required packages
            required_packages = [
                'torch', 'transformers', 'accelerate', 'flash_attn',
                'qwen_vl_utils', 'decord', 'Levenshtein', 'ray'
            ]
            
            missing_packages = []
            for package in required_packages:
                try:
                    __import__(package)
                    validation_results[f'package_{package}'] = True
                except ImportError:
                    validation_results[f'package_{package}'] = False
                    missing_packages.append(package)
            
            validation_results['all_packages_available'] = len(missing_packages) == 0
            
            if missing_packages:
                logger.warning(f"Missing packages: {missing_packages}")
            else:
                logger.info("‚úÖ All required packages available")
            
            # Check model files
            model_paths = [
                self.project_root / "models" / "spatial_mllm",
                self.project_root / "scripts" / "spatial_preprocessing.py",
                self.project_root / "scripts" / "spatial_model_validation.py"
            ]
            
            for model_path in model_paths:
                validation_results[f'path_{model_path.name}'] = model_path.exists()
                logger.info(f"Path {model_path.name}: {'‚úÖ' if model_path.exists() else '‚ùå'}")
            
            self.results['environment_validation'] = validation_results
            return validation_results
            
        except Exception as e:
            logger.error(f"Environment validation failed: {str(e)}")
            validation_results['validation_error'] = str(e)
            self.results['environment_validation'] = validation_results
            return validation_results
    
    def test_spatial_features(self) -> Dict[str, Any]:
        """Test spatial feature extraction and processing."""
        logger.info("üß† Testing spatial features...")
        test_results = {}
        
        try:
            # Import spatial preprocessing
            from scripts.spatial_preprocessing import SpatialPreprocessor
            
            # Initialize processor
            processor = SpatialPreprocessor()
            test_results['processor_initialization'] = True
            
            # Test with sample data
            sample_image_path = self.project_root / "test_data" / "sample_pizza.jpg"
            
            if sample_image_path.exists():
                # Test preprocessing
                start_time = time.time()
                spatial_data = processor.process_image(str(sample_image_path))
                processing_time = time.time() - start_time
                
                test_results['preprocessing_successful'] = spatial_data is not None
                test_results['processing_time_ms'] = processing_time * 1000
                test_results['processing_within_threshold'] = processing_time < 2.0
                
                if spatial_data:
                    # Validate spatial data structure
                    expected_keys = ['visual_features', 'spatial_features', 'metadata']
                    test_results['spatial_data_structure'] = all(key in spatial_data for key in expected_keys)
                    
                    # Test tensor shapes
                    if 'visual_features' in spatial_data:
                        visual_shape = spatial_data['visual_features'].shape
                        test_results['visual_tensor_shape_valid'] = len(visual_shape) == 4
                    
                    if 'spatial_features' in spatial_data:
                        spatial_shape = spatial_data['spatial_features'].shape
                        test_results['spatial_tensor_shape_valid'] = len(spatial_shape) == 4
                
                logger.info(f"‚úÖ Spatial preprocessing completed in {processing_time:.3f}s")
            else:
                logger.warning("Sample image not found, skipping preprocessing test")
                test_results['sample_image_available'] = False
            
            # Test synthetic data generation
            try:
                synthetic_data = processor.generate_synthetic_depth_map((224, 224))
                test_results['synthetic_depth_generation'] = synthetic_data is not None
                logger.info("‚úÖ Synthetic depth map generation successful")
            except Exception as e:
                test_results['synthetic_depth_generation'] = False
                logger.error(f"Synthetic depth generation failed: {str(e)}")
            
            self.results['spatial_features'] = test_results
            return test_results
            
        except Exception as e:
            logger.error(f"Spatial features test failed: {str(e)}")
            test_results['test_error'] = str(e)
            self.results['spatial_features'] = test_results
            return test_results
    
    def test_model_versioning(self) -> Dict[str, Any]:
        """Test spatial model versioning and compatibility."""
        logger.info("üì¶ Testing model versioning...")
        test_results = {}
        
        try:
            # Import model versioning
            from scripts.spatial_model_versioning import SpatialModelVersionManager
            
            # Initialize version manager
            version_manager = SpatialModelVersionManager()
            test_results['version_manager_initialization'] = True
            
            # Test version validation
            model_versions = version_manager.list_available_versions()
            test_results['model_versions_available'] = len(model_versions) > 0
            test_results['version_count'] = len(model_versions)
            
            logger.info(f"Available model versions: {len(model_versions)}")
            
            # Test compatibility check
            for version in model_versions[:3]:  # Test first 3 versions
                try:
                    compatibility = version_manager.check_compatibility(version)
                    test_results[f'version_{version}_compatible'] = compatibility
                    logger.info(f"Version {version} compatibility: {'‚úÖ' if compatibility else '‚ùå'}")
                except Exception as e:
                    test_results[f'version_{version}_error'] = str(e)
                    logger.warning(f"Version {version} check failed: {str(e)}")
            
            # Test model loading (lightweight check)
            if model_versions:
                latest_version = model_versions[0]
                try:
                    model_info = version_manager.get_model_info(latest_version)
                    test_results['model_info_retrieval'] = model_info is not None
                    
                    if model_info:
                        test_results['model_has_metadata'] = 'metadata' in model_info
                        test_results['model_has_checksum'] = 'checksum' in model_info
                        logger.info(f"‚úÖ Model info retrieved for {latest_version}")
                    
                except Exception as e:
                    test_results['model_info_error'] = str(e)
                    logger.warning(f"Model info retrieval failed: {str(e)}")
            
            self.results['model_versioning'] = test_results
            return test_results
            
        except Exception as e:
            logger.error(f"Model versioning test failed: {str(e)}")
            test_results['test_error'] = str(e)
            self.results['model_versioning'] = test_results
            return test_results
    
    def test_deployment_endpoints(self, environment: str = "test") -> Dict[str, Any]:
        """Test deployment endpoints for spatial API."""
        logger.info(f"üåê Testing deployment endpoints for {environment}...")
        test_results = {}
        
        try:
            import requests
            import time
            
            # Define test endpoints
            base_urls = {
                'development': 'http://localhost:8001',
                'staging': 'http://localhost:8001',
                'test': 'http://localhost:8001',
                'production': 'https://api.pizza-detection.com'
            }
            
            base_url = base_urls.get(environment, 'http://localhost:8001')
            
            endpoints_to_test = [
                '/health',
                '/api/v1/spatial/info',
                '/api/v1/spatial/models',
                '/api/v1/classify/spatial'
            ]
            
            # Wait for service to be ready
            max_retries = 30
            for attempt in range(max_retries):
                try:
                    response = requests.get(f"{base_url}/health", timeout=5)
                    if response.status_code == 200:
                        break
                except requests.exceptions.RequestException:
                    pass
                
                if attempt < max_retries - 1:
                    time.sleep(2)
                else:
                    test_results['service_startup_timeout'] = True
                    logger.warning("Service startup timeout")
                    return test_results
            
            # Test each endpoint
            for endpoint in endpoints_to_test:
                try:
                    start_time = time.time()
                    response = requests.get(f"{base_url}{endpoint}", timeout=10)
                    response_time = time.time() - start_time
                    
                    test_results[f'endpoint_{endpoint.replace("/", "_")}_status'] = response.status_code
                    test_results[f'endpoint_{endpoint.replace("/", "_")}_response_time'] = response_time
                    test_results[f'endpoint_{endpoint.replace("/", "_")}_success'] = response.status_code == 200
                    
                    logger.info(f"Endpoint {endpoint}: {response.status_code} ({response_time:.3f}s)")
                    
                except requests.exceptions.RequestException as e:
                    test_results[f'endpoint_{endpoint.replace("/", "_")}_error'] = str(e)
                    logger.warning(f"Endpoint {endpoint} failed: {str(e)}")
            
            # Test spatial classification endpoint with sample data
            if '/api/v1/classify/spatial' in endpoints_to_test:
                try:
                    # Prepare test data
                    test_image_path = self.project_root / "test_data" / "sample_pizza.jpg"
                    
                    if test_image_path.exists():
                        with open(test_image_path, 'rb') as f:
                            files = {'image': f}
                            start_time = time.time()
                            response = requests.post(f"{base_url}/api/v1/classify/spatial", 
                                                   files=files, timeout=30)
                            response_time = time.time() - start_time
                        
                        test_results['spatial_classification_status'] = response.status_code
                        test_results['spatial_classification_time'] = response_time
                        test_results['spatial_classification_success'] = response.status_code == 200
                        
                        if response.status_code == 200:
                            try:
                                result_data = response.json()
                                test_results['spatial_classification_has_result'] = 'classification' in result_data
                                test_results['spatial_classification_has_confidence'] = 'confidence' in result_data
                                test_results['spatial_classification_has_spatial_features'] = 'spatial_features' in result_data
                                logger.info("‚úÖ Spatial classification endpoint working")
                            except json.JSONDecodeError:
                                test_results['spatial_classification_json_error'] = True
                                logger.warning("Spatial classification response not valid JSON")
                        
                        logger.info(f"Spatial classification: {response.status_code} ({response_time:.3f}s)")
                    else:
                        test_results['test_image_missing'] = True
                        logger.warning("Test image not available for classification test")
                        
                except requests.exceptions.RequestException as e:
                    test_results['spatial_classification_error'] = str(e)
                    logger.warning(f"Spatial classification test failed: {str(e)}")
            
            self.results['deployment_endpoints'] = test_results
            return test_results
            
        except Exception as e:
            logger.error(f"Deployment endpoints test failed: {str(e)}")
            test_results['test_error'] = str(e)
            self.results['deployment_endpoints'] = test_results
            return test_results
    
    def test_docker_environment(self) -> Dict[str, Any]:
        """Test Docker-specific spatial functionality."""
        logger.info("üê≥ Testing Docker environment...")
        test_results = {}
        
        try:
            # Check if running in Docker
            test_results['running_in_docker'] = self.docker_mode or os.path.exists('/.dockerenv')
            
            # Test GPU access in Docker
            if torch.cuda.is_available():
                test_results['docker_gpu_access'] = True
                gpu_name = torch.cuda.get_device_properties(0).name
                test_results['docker_gpu_name'] = gpu_name
                logger.info(f"‚úÖ GPU accessible in Docker: {gpu_name}")
            else:
                test_results['docker_gpu_access'] = False
                logger.info("GPU not accessible in Docker (CPU mode)")
            
            # Test model loading in Docker environment
            try:
                from transformers import AutoTokenizer
                tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-7B", trust_remote_code=True)
                test_results['docker_model_loading'] = tokenizer is not None
                logger.info("‚úÖ Model loading successful in Docker")
            except Exception as e:
                test_results['docker_model_loading'] = False
                test_results['docker_model_error'] = str(e)
                logger.warning(f"Model loading failed in Docker: {str(e)}")
            
            # Test memory allocation
            try:
                # Test tensor allocation
                test_tensor = torch.randn(1000, 1000, device=self.device)
                memory_allocated = torch.cuda.memory_allocated() if torch.cuda.is_available() else 0
                test_results['docker_memory_allocation'] = True
                test_results['docker_memory_allocated_mb'] = memory_allocated / (1024**2)
                del test_tensor
                logger.info(f"‚úÖ Memory allocation test passed ({memory_allocated / (1024**2):.1f} MB)")
            except Exception as e:
                test_results['docker_memory_allocation'] = False
                test_results['docker_memory_error'] = str(e)
                logger.warning(f"Memory allocation failed: {str(e)}")
            
            # Test file system permissions
            try:
                test_file = Path("/app/test_permissions.txt")
                test_file.write_text("Docker permission test")
                test_results['docker_file_permissions'] = test_file.exists()
                test_file.unlink()
                logger.info("‚úÖ File system permissions OK")
            except Exception as e:
                test_results['docker_file_permissions'] = False
                test_results['docker_file_error'] = str(e)
                logger.warning(f"File system permission test failed: {str(e)}")
            
            self.results['docker_environment'] = test_results
            return test_results
            
        except Exception as e:
            logger.error(f"Docker environment test failed: {str(e)}")
            test_results['test_error'] = str(e)
            self.results['docker_environment'] = test_results
            return test_results
    
    def test_performance_metrics(self) -> Dict[str, Any]:
        """Test performance metrics for deployment validation."""
        logger.info("‚ö° Testing performance metrics...")
        test_results = {}
        
        try:
            # Test inference speed
            if torch.cuda.is_available():
                # Warm up GPU
                dummy_tensor = torch.randn(1, 3, 224, 224, device=self.device)
                for _ in range(5):
                    _ = torch.nn.functional.relu(dummy_tensor)
                
                # Measure inference time
                inference_times = []
                for i in range(10):
                    start_time = time.time()
                    # Simulate spatial processing
                    visual_features = torch.randn(1, 1, 3, 518, 518, device=self.device)
                    spatial_features = torch.randn(1, 1, 4, 518, 518, device=self.device)
                    combined = torch.cat([visual_features, spatial_features], dim=2)
                    result = torch.nn.functional.adaptive_avg_pool2d(combined.squeeze(0).squeeze(0), (1, 1))
                    torch.cuda.synchronize()
                    inference_time = time.time() - start_time
                    inference_times.append(inference_time * 1000)  # Convert to ms
                
                avg_inference_time = sum(inference_times) / len(inference_times)
                max_inference_time = max(inference_times)
                min_inference_time = min(inference_times)
                
                test_results['avg_inference_time_ms'] = avg_inference_time
                test_results['max_inference_time_ms'] = max_inference_time
                test_results['min_inference_time_ms'] = min_inference_time
                test_results['inference_within_threshold'] = avg_inference_time < self.test_config['max_inference_time_ms']
                
                logger.info(f"Average inference time: {avg_inference_time:.2f}ms")
                logger.info(f"Min/Max inference time: {min_inference_time:.2f}/{max_inference_time:.2f}ms")
                
                # Test memory usage
                torch.cuda.reset_peak_memory_stats()
                large_tensor = torch.randn(100, 100, 512, 512, device=self.device)
                peak_memory = torch.cuda.max_memory_allocated() / (1024**3)  # GB
                test_results['peak_memory_usage_gb'] = peak_memory
                test_results['memory_within_threshold'] = peak_memory < self.test_config['max_memory_usage_gb']
                del large_tensor
                
                logger.info(f"Peak memory usage: {peak_memory:.2f} GB")
            
            else:
                # CPU-only testing
                test_results['cpu_mode'] = True
                inference_times = []
                for i in range(5):  # Fewer iterations for CPU
                    start_time = time.time()
                    dummy_tensor = torch.randn(1, 3, 224, 224)
                    result = torch.nn.functional.relu(dummy_tensor)
                    inference_time = time.time() - start_time
                    inference_times.append(inference_time * 1000)
                
                avg_inference_time = sum(inference_times) / len(inference_times)
                test_results['cpu_avg_inference_time_ms'] = avg_inference_time
                logger.info(f"CPU average inference time: {avg_inference_time:.2f}ms")
            
            # Test throughput
            batch_sizes = [1, 2, 4]
            for batch_size in batch_sizes:
                try:
                    start_time = time.time()
                    batch_tensor = torch.randn(batch_size, 3, 224, 224, device=self.device)
                    result = torch.nn.functional.relu(batch_tensor)
                    if torch.cuda.is_available():
                        torch.cuda.synchronize()
                    batch_time = time.time() - start_time
                    
                    throughput = batch_size / batch_time
                    test_results[f'throughput_batch_{batch_size}'] = throughput
                    logger.info(f"Batch {batch_size} throughput: {throughput:.2f} images/sec")
                    
                except Exception as e:
                    test_results[f'throughput_batch_{batch_size}_error'] = str(e)
                    logger.warning(f"Batch {batch_size} throughput test failed: {str(e)}")
            
            self.results['performance_metrics'] = test_results
            return test_results
            
        except Exception as e:
            logger.error(f"Performance metrics test failed: {str(e)}")
            test_results['test_error'] = str(e)
            self.results['performance_metrics'] = test_results
            return test_results
    
    def generate_deployment_report(self, output_path: Optional[str] = None) -> Dict[str, Any]:
        """Generate comprehensive deployment report."""
        logger.info("üìä Generating deployment report...")
        
        report = {
            'timestamp': datetime.now().isoformat(),
            'test_duration_seconds': time.time() - self.start_time,
            'environment': {
                'docker_mode': self.docker_mode,
                'device': str(self.device),
                'cuda_available': torch.cuda.is_available(),
                'python_version': f"{sys.version_info.major}.{sys.version_info.minor}.{sys.version_info.micro}"
            },
            'test_results': self.results,
            'summary': {}
        }
        
        # Calculate summary statistics
        total_tests = 0
        passed_tests = 0
        failed_tests = 0
        
        for test_category, test_data in self.results.items():
            if isinstance(test_data, dict):
                for test_name, test_result in test_data.items():
                    if isinstance(test_result, bool):
                        total_tests += 1
                        if test_result:
                            passed_tests += 1
                        else:
                            failed_tests += 1
        
        report['summary'] = {
            'total_tests': total_tests,
            'passed_tests': passed_tests,
            'failed_tests': failed_tests,
            'success_rate': passed_tests / total_tests if total_tests > 0 else 0,
            'deployment_ready': failed_tests == 0 and passed_tests > 0
        }
        
        # Save report
        if output_path is None:
            output_path = self.project_root / "output" / "spatial_deployment_report.json"
        
        output_path = Path(output_path)
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        with open(output_path, 'w') as f:
            json.dump(report, f, indent=2)
        
        logger.info(f"üìä Deployment report saved to {output_path}")
        logger.info(f"Summary: {passed_tests}/{total_tests} tests passed ({report['summary']['success_rate']:.1%})")
        
        return report
    
    def run_all_tests(self) -> bool:
        """Run all deployment tests."""
        logger.info("üöÄ Starting comprehensive spatial deployment tests...")
        
        try:
            # Run validation tests
            env_results = self.validate_environment()
            feature_results = self.test_spatial_features()
            versioning_results = self.test_model_versioning()
            
            if self.docker_mode:
                docker_results = self.test_docker_environment()
            
            performance_results = self.test_performance_metrics()
            
            # Generate final report
            report = self.generate_deployment_report()
            
            success = report['summary']['deployment_ready']
            
            if success:
                logger.info("üéâ All spatial deployment tests passed!")
            else:
                logger.error("‚ùå Some spatial deployment tests failed!")
            
            return success
            
        except Exception as e:
            logger.error(f"Deployment test suite failed: {str(e)}")
            logger.error(traceback.format_exc())
            return False

def main():
    """Main function for command-line usage."""
    parser = argparse.ArgumentParser(description="Enhanced Spatial-MLLM Deployment Tests")
    parser.add_argument("--validate-env", action="store_true", help="Validate environment only")
    parser.add_argument("--test-features", action="store_true", help="Test spatial features only")
    parser.add_argument("--test-preprocessing", action="store_true", help="Test preprocessing only")
    parser.add_argument("--test-inference", action="store_true", help="Test inference only")
    parser.add_argument("--test-endpoints", action="store_true", help="Test deployment endpoints")
    parser.add_argument("--docker-mode", action="store_true", help="Run in Docker mode")
    parser.add_argument("--environment", default="test", help="Target environment")
    parser.add_argument("--output", help="Output path for report")
    parser.add_argument("--dry-run", action="store_true", help="Perform dry run validation without executing tests")
    
    args = parser.parse_args()
    
    # Handle dry-run mode
    if args.dry_run:
        logger.info("üß™ Running in DRY-RUN mode - validation only")
        logger.info("‚úÖ Script syntax is valid")
        logger.info("‚úÖ All imports successful")
        logger.info("‚úÖ Configuration loaded")
        logger.info("‚úÖ Dry-run validation completed successfully")
        sys.exit(0)
    
    # Initialize test suite
    test_suite = EnhancedSpatialDeploymentTests(docker_mode=args.docker_mode)
    
    success = True
    
    try:
        if args.validate_env:
            results = test_suite.validate_environment()
            success = all(v for v in results.values() if isinstance(v, bool))
        elif args.test_features:
            results = test_suite.test_spatial_features()
            success = results.get('processor_initialization', False)
        elif args.test_preprocessing:
            results = test_suite.test_spatial_features()
            success = results.get('preprocessing_successful', False)
        elif args.test_inference:
            results = test_suite.test_performance_metrics()
            success = results.get('inference_within_threshold', False)
        elif args.test_endpoints:
            results = test_suite.test_deployment_endpoints(args.environment)
            success = any(v for k, v in results.items() if k.endswith('_success') and isinstance(v, bool))
        else:
            # Run all tests
            success = test_suite.run_all_tests()
        
        # Generate report
        if args.output:
            test_suite.generate_deployment_report(args.output)
        
    except Exception as e:
        logger.error(f"Test execution failed: {str(e)}")
        success = False
    
    sys.exit(0 if success else 1)

if __name__ == "__main__":
    main()
