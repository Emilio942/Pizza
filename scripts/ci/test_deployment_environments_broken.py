#!/usr/bin/env python3
"""
SPATIAL-4.2: Deployment Environment Testing

Comprehensive testing script for validating Spatial-MLLM deployment across
different environments (development, staging, production).
"""

import os
import sys
import json
import time
import logging
import requests
import subprocess
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Any
import argparse
from concurrent.futures import ThreadPoolExecutor, as_completed

import docker
import torch

# Set up logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class DeploymentEnvironmentTester:
    """Test Spatial-MLLM deployment across different environments."""
    
    def __init__(self, project_root: str = None):
        """Initialize the deployment tester."""
        self.project_root = Path(project_root) if project_root else Path(__file__).parent.parent.parent
        self.docker_client = None
        self.test_results = {}
        
        # Initialize Docker client
        try:
            self.docker_client = docker.from_env()
            logger.info("üê≥ Docker client initialized")\n        except Exception as e:\n            logger.warning(f"Docker client initialization failed: {str(e)}")\n        \n        # Environment configurations\n        self.environments = {\n            'development': {\n                'compose_file': 'docker-compose.yml',\n                'services': ['pizza-api-spatial', 'redis'],\n                'api_port': 8001,\n                'timeout': 60,\n                'health_check_retries': 10,\n                'performance_requirements': {\n                    'response_time_ms': 3000,\n                    'availability_percent': 95\n                }\n            },\n            'staging': {\n                'compose_file': 'docker-compose.yml',\n                'services': ['pizza-api-spatial', 'redis', 'nginx'],\n                'api_port': 8001,\n                'timeout': 120,\n                'health_check_retries': 15,\n                'performance_requirements': {\n                    'response_time_ms': 2000,\n                    'availability_percent': 98\n                }\n            },\n            'production': {\n                'compose_file': 'docker-compose.yml',\n                'services': ['pizza-api-spatial', 'redis', 'nginx', 'prometheus', 'grafana'],\n                'api_port': 8001,\n                'timeout': 180,\n                'health_check_retries': 20,\n                'performance_requirements': {\n                    'response_time_ms': 1500,\n                    'availability_percent': 99.5\n                }\n            }\n        }\n        \n        logger.info("üß™ Deployment Environment Tester initialized")\n    \n    def setup_environment(self, environment: str) -> bool:\n        \"\"\"Set up a specific deployment environment.\"\"\"\n        logger.info(f"üîß Setting up {environment} environment...")\n        \n        if environment not in self.environments:\n            logger.error(f"Unknown environment: {environment}\")\n            return False\n        \n        env_config = self.environments[environment]\n        \n        try:\n            # Set environment variables\n            os.environ['ENVIRONMENT'] = environment\n            os.environ['MODEL_TYPE'] = 'spatial'\n            os.environ['LOG_LEVEL'] = 'INFO'\n            \n            # Clean up any existing containers\n            self.cleanup_environment(environment)\n            \n            # Start services using docker-compose\n            compose_cmd = [\n                'docker-compose',\n                '-f', env_config['compose_file'],\n                'up', '-d'\n            ] + env_config['services']\n            \n            logger.info(f"Starting services: {env_config['services']}\")\n            result = subprocess.run(compose_cmd, \n                                  cwd=self.project_root,\n                                  capture_output=True, \n                                  text=True,\n                                  timeout=env_config['timeout'])\n            \n            if result.returncode != 0:\n                logger.error(f"Failed to start services: {result.stderr}\")\n                return False\n            \n            # Wait for services to be ready\n            if not self.wait_for_services(environment):\n                logger.error(f"Services not ready in time for {environment}\")\n                return False\n            \n            logger.info(f"‚úÖ {environment} environment setup successful")\n            return True\n            \n        except subprocess.TimeoutExpired:\n            logger.error(f"Timeout setting up {environment} environment")\n            return False\n        except Exception as e:\n            logger.error(f"Failed to setup {environment} environment: {str(e)}\")\n            return False\n    \n    def wait_for_services(self, environment: str) -> bool:\n        \"\"\"Wait for services to be ready.\"\"\"\n        env_config = self.environments[environment]\n        api_port = env_config['api_port']\n        max_retries = env_config['health_check_retries']\n        \n        logger.info(f"‚è≥ Waiting for services to be ready...")\n        \n        for attempt in range(max_retries):\n            try:\n                # Check API health\n                response = requests.get(f\"http://localhost:{api_port}/health\", \n                                      timeout=10)\n                \n                if response.status_code == 200:\n                    logger.info(f\"‚úÖ Services ready after {attempt + 1} attempts")\n                    return True\n            \n            except requests.exceptions.RequestException:\n                pass\n            \n            if attempt < max_retries - 1:\n                time.sleep(6)  # Wait 6 seconds between attempts\n                logger.info(f\"Attempt {attempt + 1}/{max_retries} - services not ready yet...")\n        \n        logger.error(f"Services not ready after {max_retries} attempts")\n        return False\n    \n    def test_spatial_api_endpoints(self, environment: str) -> Dict[str, Any]:\n        \"\"\"Test Spatial-MLLM API endpoints.\"\"\"\n        logger.info(f"üåê Testing spatial API endpoints for {environment}...")\n        \n        env_config = self.environments[environment]\n        api_port = env_config['api_port']\n        base_url = f\"http://localhost:{api_port}\"\n        \n        test_results = {\n            'environment': environment,\n            'test_timestamp': time.time(),\n            'endpoints': {}\n        }\n        \n        # Define test endpoints\n        endpoints = [\n            {\n                'name': 'health_check',\n                'method': 'GET',\n                'path': '/health',\n                'expected_status': 200,\n                'timeout': 10\n            },\n            {\n                'name': 'spatial_info',\n                'method': 'GET',\n                'path': '/api/v1/spatial/info',\n                'expected_status': 200,\n                'timeout': 15\n            },\n            {\n                'name': 'spatial_models',\n                'method': 'GET',\n                'path': '/api/v1/spatial/models',\n                'expected_status': 200,\n                'timeout': 15\n            },\n            {\n                'name': 'model_versions',\n                'method': 'GET',\n                'path': '/api/v1/models/versions',\n                'expected_status': 200,\n                'timeout': 10\n            }\n        ]\n        \n        # Test each endpoint\n        for endpoint in endpoints:\n            endpoint_name = endpoint['name']\n            logger.info(f\"Testing endpoint: {endpoint['path']}\")\n            \n            try:\n                start_time = time.time()\n                \n                if endpoint['method'] == 'GET':\n                    response = requests.get(\n                        f\"{base_url}{endpoint['path']}\",\n                        timeout=endpoint['timeout']\n                    )\n                elif endpoint['method'] == 'POST':\n                    response = requests.post(\n                        f\"{base_url}{endpoint['path']}\",\n                        timeout=endpoint['timeout']\n                    )\n                \n                response_time = (time.time() - start_time) * 1000  # ms\n                \n                test_results['endpoints'][endpoint_name] = {\n                    'status_code': response.status_code,\n                    'response_time_ms': response_time,\n                    'success': response.status_code == endpoint['expected_status'],\n                    'content_length': len(response.content),\n                    'headers': dict(response.headers)\n                }\n                \n                # Try to parse JSON response\n                try:\n                    json_data = response.json()\n                    test_results['endpoints'][endpoint_name]['json_valid'] = True\n                    test_results['endpoints'][endpoint_name]['response_data'] = json_data\n                except json.JSONDecodeError:\n                    test_results['endpoints'][endpoint_name]['json_valid'] = False\n                \n                logger.info(f\"  ‚úÖ {endpoint_name}: {response.status_code} ({response_time:.1f}ms)\")\n                \n            except requests.exceptions.RequestException as e:\n                test_results['endpoints'][endpoint_name] = {\n                    'error': str(e),\n                    'success': False\n                }\n                logger.error(f\"  ‚ùå {endpoint_name}: {str(e)}\")\n        \n        return test_results\n    \n    def test_spatial_classification(self, environment: str) -> Dict[str, Any]:\n        \"\"\"Test spatial classification functionality.\"\"\"\n        logger.info(f\"üß† Testing spatial classification for {environment}...\")\n        \n        env_config = self.environments[environment]\n        api_port = env_config['api_port']\n        base_url = f\"http://localhost:{api_port}\"\n        \n        test_results = {\n            'environment': environment,\n            'classification_tests': {}\n        }\n        \n        # Test images\n        test_images = [\n            'test_data/sample_pizza.jpg',\n            'test_data/pizza_burnt.jpg',\n            'test_data/pizza_perfect.jpg'\n        ]\n        \n        for i, image_path in enumerate(test_images):\n            test_name = f\"test_image_{i + 1}\"\n            image_file = self.project_root / image_path\n            \n            if not image_file.exists():\n                test_results['classification_tests'][test_name] = {\n                    'error': f\"Test image not found: {image_path}\",\n                    'success': False\n                }\n                continue\n            \n            try:\n                start_time = time.time()\n                \n                with open(image_file, 'rb') as f:\n                    files = {'image': f}\n                    response = requests.post(\n                        f\"{base_url}/api/v1/classify/spatial\",\n                        files=files,\n                        timeout=30\n                    )\n                \n                response_time = (time.time() - start_time) * 1000  # ms\n                \n                test_results['classification_tests'][test_name] = {\n                    'status_code': response.status_code,\n                    'response_time_ms': response_time,\n                    'success': response.status_code == 200\n                }\n                \n                if response.status_code == 200:\n                    try:\n                        result_data = response.json()\n                        test_results['classification_tests'][test_name].update({\n                            'classification': result_data.get('classification'),\n                            'confidence': result_data.get('confidence'),\n                            'has_spatial_features': 'spatial_features' in result_data,\n                            'spatial_feature_count': len(result_data.get('spatial_features', {})),\n                            'model_version': result_data.get('model_version')\n                        })\n                        \n                        logger.info(f\"  ‚úÖ {test_name}: {result_data.get('classification')} ({result_data.get('confidence', 0):.2f})\")\n                        \n                    except json.JSONDecodeError:\n                        test_results['classification_tests'][test_name]['json_error'] = True\n                        logger.warning(f\"  ‚ö†Ô∏è {test_name}: Invalid JSON response\")\n                else:\n                    logger.error(f\"  ‚ùå {test_name}: HTTP {response.status_code}\")\n                \n            except requests.exceptions.RequestException as e:\n                test_results['classification_tests'][test_name] = {\n                    'error': str(e),\n                    'success': False\n                }\n                logger.error(f\"  ‚ùå {test_name}: {str(e)}\")\n        \n        return test_results\n    \n    def test_performance_metrics(self, environment: str) -> Dict[str, Any]:\n        \"\"\"Test performance metrics for the environment.\"\"\"\n        logger.info(f\"‚ö° Testing performance metrics for {environment}...\")\n        \n        env_config = self.environments[environment]\n        api_port = env_config['api_port']\n        base_url = f\"http://localhost:{api_port}\"\n        \n        test_results = {\n            'environment': environment,\n            'performance_tests': {}\n        }\n        \n        # Load test - multiple concurrent requests\n        logger.info(\"Running concurrent load test...\")\n        \n        def make_request(request_id: int) -> Dict[str, Any]:\n            \"\"\"Make a single API request.\"\"\"\n            try:\n                start_time = time.time()\n                response = requests.get(f\"{base_url}/health\", timeout=10)\n                response_time = (time.time() - start_time) * 1000\n                \n                return {\n                    'request_id': request_id,\n                    'status_code': response.status_code,\n                    'response_time_ms': response_time,\n                    'success': response.status_code == 200\n                }\n            except Exception as e:\n                return {\n                    'request_id': request_id,\n                    'error': str(e),\n                    'success': False\n                }\n        \n        # Run concurrent requests\n        concurrent_requests = 10\n        request_results = []\n        \n        with ThreadPoolExecutor(max_workers=concurrent_requests) as executor:\n            futures = [executor.submit(make_request, i) for i in range(concurrent_requests)]\n            \n            for future in as_completed(futures):\n                request_results.append(future.result())\n        \n        # Analyze results\n        successful_requests = [r for r in request_results if r.get('success', False)]\n        failed_requests = [r for r in request_results if not r.get('success', False)]\n        \n        if successful_requests:\n            response_times = [r['response_time_ms'] for r in successful_requests]\n            \n            test_results['performance_tests']['load_test'] = {\n                'total_requests': len(request_results),\n                'successful_requests': len(successful_requests),\n                'failed_requests': len(failed_requests),\n                'success_rate': len(successful_requests) / len(request_results),\n                'avg_response_time_ms': sum(response_times) / len(response_times),\n                'min_response_time_ms': min(response_times),\n                'max_response_time_ms': max(response_times),\n                'meets_requirements': True\n            }\n            \n            # Check against requirements\n            avg_response_time = test_results['performance_tests']['load_test']['avg_response_time_ms']\n            success_rate = test_results['performance_tests']['load_test']['success_rate'] * 100\n            \n            required_response_time = env_config['performance_requirements']['response_time_ms']\n            required_availability = env_config['performance_requirements']['availability_percent']\n            \n            meets_response_time = avg_response_time <= required_response_time\n            meets_availability = success_rate >= required_availability\n            \n            test_results['performance_tests']['load_test'].update({\n                'meets_response_time_requirement': meets_response_time,\n                'meets_availability_requirement': meets_availability,\n                'meets_requirements': meets_response_time and meets_availability\n            })\n            \n            logger.info(f\"  Load test: {len(successful_requests)}/{len(request_results)} requests successful\")\n            logger.info(f\"  Avg response time: {avg_response_time:.1f}ms (req: <{required_response_time}ms)\")\n            logger.info(f\"  Availability: {success_rate:.1f}% (req: >{required_availability}%)\")\n        \n        # Memory usage test (if available)\n        try:\n            response = requests.get(f\"{base_url}/api/v1/system/metrics\", timeout=10)\n            if response.status_code == 200:\n                metrics = response.json()\n                test_results['performance_tests']['memory_usage'] = {\n                    'memory_usage_mb': metrics.get('memory_usage_mb', 0),\n                    'gpu_memory_usage_mb': metrics.get('gpu_memory_usage_mb', 0),\n                    'cpu_usage_percent': metrics.get('cpu_usage_percent', 0)\n                }\n                logger.info(f\"  Memory usage: {metrics.get('memory_usage_mb', 0):.1f}MB\")\n        except Exception as e:\n            logger.warning(f\"Could not retrieve system metrics: {str(e)}\")\n        \n        return test_results\n    \n    def test_model_versioning(self, environment: str) -> Dict[str, Any]:\n        \"\"\"Test model versioning functionality.\"\"\"\n        logger.info(f\"üì¶ Testing model versioning for {environment}...\")\n        \n        env_config = self.environments[environment]\n        api_port = env_config['api_port']\n        base_url = f\"http://localhost:{api_port}\"\n        \n        test_results = {\n            'environment': environment,\n            'versioning_tests': {}\n        }\n        \n        # Test version listing\n        try:\n            response = requests.get(f\"{base_url}/api/v1/models/versions\", timeout=15)\n            \n            test_results['versioning_tests']['list_versions'] = {\n                'status_code': response.status_code,\n                'success': response.status_code == 200\n            }\n            \n            if response.status_code == 200:\n                versions_data = response.json()\n                test_results['versioning_tests']['list_versions'].update({\n                    'version_count': len(versions_data.get('versions', [])),\n                    'has_current_version': 'current_version' in versions_data,\n                    'has_available_versions': len(versions_data.get('versions', [])) > 0\n                })\n                logger.info(f\"  ‚úÖ Version listing: {len(versions_data.get('versions', []))} versions available\")\n        \n        except Exception as e:\n            test_results['versioning_tests']['list_versions'] = {\n                'error': str(e),\n                'success': False\n            }\n            logger.error(f\"  ‚ùå Version listing failed: {str(e)}\")\n        \n        # Test version validation\n        try:\n            response = requests.get(f\"{base_url}/api/v1/models/validate\", timeout=20)\n            \n            test_results['versioning_tests']['validate_current'] = {\n                'status_code': response.status_code,\n                'success': response.status_code == 200\n            }\n            \n            if response.status_code == 200:\n                validation_data = response.json()\n                test_results['versioning_tests']['validate_current'].update({\n                    'validation_passed': validation_data.get('validation_passed', False),\n                    'compatibility_check': validation_data.get('compatibility_check', False),\n                    'performance_check': validation_data.get('performance_check', False)\n                })\n                logger.info(f\"  ‚úÖ Model validation: {'PASSED' if validation_data.get('validation_passed') else 'FAILED'}\")\n        \n        except Exception as e:\n            test_results['versioning_tests']['validate_current'] = {\n                'error': str(e),\n                'success': False\n            }\n            logger.error(f\"  ‚ùå Model validation failed: {str(e)}\")\n        \n        return test_results\n    \n    def cleanup_environment(self, environment: str) -> bool:\n        \"\"\"Clean up environment resources.\"\"\"\n        logger.info(f\"üßπ Cleaning up {environment} environment...\")\n        \n        if environment not in self.environments:\n            return False\n        \n        env_config = self.environments[environment]\n        \n        try:\n            # Stop services using docker-compose\n            compose_cmd = [\n                'docker-compose',\n                '-f', env_config['compose_file'],\n                'down', '-v'\n            ]\n            \n            result = subprocess.run(compose_cmd,\n                                  cwd=self.project_root,\n                                  capture_output=True,\n                                  text=True,\n                                  timeout=60)\n            \n            if result.returncode == 0:\n                logger.info(f\"‚úÖ {environment} environment cleaned up\")\n                return True\n            else:\n                logger.warning(f\"Cleanup warnings for {environment}: {result.stderr}\")\n                return True  # Non-critical warnings\n        \n        except Exception as e:\n            logger.error(f\"Failed to cleanup {environment}: {str(e)}\")\n            return False\n    \n    def run_comprehensive_test(self, environment: str) -> Dict[str, Any]:\n        \"\"\"Run comprehensive test suite for an environment.\"\"\"\n        logger.info(f\"üöÄ Running comprehensive test for {environment}...\")\n        \n        start_time = time.time()\n        test_results = {\n            'environment': environment,\n            'start_time': start_time,\n            'test_phases': {}\n        }\n        \n        try:\n            # Phase 1: Environment setup\n            logger.info(\"Phase 1: Environment Setup\")\n            setup_success = self.setup_environment(environment)\n            test_results['test_phases']['setup'] = {\n                'success': setup_success,\n                'duration_seconds': time.time() - start_time\n            }\n            \n            if not setup_success:\n                logger.error(f\"Environment setup failed for {environment}\")\n                return test_results\n            \n            # Phase 2: API endpoint testing\n            logger.info(\"Phase 2: API Endpoint Testing\")\n            phase_start = time.time()\n            api_results = self.test_spatial_api_endpoints(environment)\n            test_results['test_phases']['api_endpoints'] = api_results\n            test_results['test_phases']['api_endpoints']['duration_seconds'] = time.time() - phase_start\n            \n            # Phase 3: Classification testing\n            logger.info(\"Phase 3: Classification Testing\")\n            phase_start = time.time()\n            classification_results = self.test_spatial_classification(environment)\n            test_results['test_phases']['classification'] = classification_results\n            test_results['test_phases']['classification']['duration_seconds'] = time.time() - phase_start\n            \n            # Phase 4: Performance testing\n            logger.info(\"Phase 4: Performance Testing\")\n            phase_start = time.time()\n            performance_results = self.test_performance_metrics(environment)\n            test_results['test_phases']['performance'] = performance_results\n            test_results['test_phases']['performance']['duration_seconds'] = time.time() - phase_start\n            \n            # Phase 5: Model versioning testing\n            logger.info(\"Phase 5: Model Versioning Testing\")\n            phase_start = time.time()\n            versioning_results = self.test_model_versioning(environment)\n            test_results['test_phases']['versioning'] = versioning_results\n            test_results['test_phases']['versioning']['duration_seconds'] = time.time() - phase_start\n            \n            # Calculate overall results\n            total_duration = time.time() - start_time\n            test_results['total_duration_seconds'] = total_duration\n            \n            # Determine overall success\n            overall_success = True\n            phase_successes = []\n            \n            for phase_name, phase_data in test_results['test_phases'].items():\n                phase_success = self.evaluate_phase_success(phase_data)\n                phase_successes.append(phase_success)\n                overall_success = overall_success and phase_success\n                logger.info(f\"  {phase_name}: {'‚úÖ PASSED' if phase_success else '‚ùå FAILED'}\")\n            \n            test_results['overall_success'] = overall_success\n            test_results['phase_success_rate'] = sum(phase_successes) / len(phase_successes)\n            \n            logger.info(f\"üéØ Comprehensive test for {environment}: {'‚úÖ PASSED' if overall_success else '‚ùå FAILED'}\")\n            logger.info(f\"‚è±Ô∏è Total duration: {total_duration:.1f} seconds\")\n            \n        except Exception as e:\n            logger.error(f\"Comprehensive test failed for {environment}: {str(e)}\")\n            test_results['error'] = str(e)\n            test_results['overall_success'] = False\n        \n        finally:\n            # Always attempt cleanup\n            self.cleanup_environment(environment)\n        \n        return test_results\n    \n    def evaluate_phase_success(self, phase_data: Dict[str, Any]) -> bool:\n        \"\"\"Evaluate if a test phase was successful.\"\"\"\n        if 'success' in phase_data:\n            return phase_data['success']\n        \n        # For complex phases, check sub-results\n        if 'endpoints' in phase_data:\n            # API endpoints phase\n            endpoint_successes = [ep_data.get('success', False) \n                                for ep_data in phase_data['endpoints'].values()]\n            return len(endpoint_successes) > 0 and sum(endpoint_successes) / len(endpoint_successes) >= 0.8\n        \n        if 'classification_tests' in phase_data:\n            # Classification phase\n            classification_successes = [test_data.get('success', False) \n                                      for test_data in phase_data['classification_tests'].values()]\n            return len(classification_successes) > 0 and sum(classification_successes) / len(classification_successes) >= 0.7\n        \n        if 'performance_tests' in phase_data:\n            # Performance phase\n            performance_tests = phase_data['performance_tests']\n            if 'load_test' in performance_tests:\n                return performance_tests['load_test'].get('meets_requirements', False)\n            return True\n        \n        if 'versioning_tests' in phase_data:\n            # Versioning phase\n            versioning_successes = [test_data.get('success', False) \n                                  for test_data in phase_data['versioning_tests'].values()]\n            return len(versioning_successes) > 0 and sum(versioning_successes) / len(versioning_successes) >= 0.5\n        \n        return True  # Default to success if we can't determine\n    \n    def generate_deployment_report(self, test_results: Dict[str, Any], output_path: str = None) -> str:\n        \"\"\"Generate comprehensive deployment test report.\"\"\"\n        if output_path is None:\n            timestamp = int(time.time())\n            output_path = self.project_root / f\"output/deployment_test_report_{timestamp}.json\"\n        \n        output_path = Path(output_path)\n        output_path.parent.mkdir(parents=True, exist_ok=True)\n        \n        # Enhance report with summary statistics\n        enhanced_report = {\n            'report_metadata': {\n                'generated_at': time.time(),\n                'tester_version': '1.0.0',\n                'project_root': str(self.project_root)\n            },\n            'test_results': test_results,\n            'summary': {\n                'environments_tested': len([r for r in test_results.values() if isinstance(r, dict)]),\n                'overall_success_rate': 0.0,\n                'total_test_duration': 0.0\n            }\n        }\n        \n        # Calculate summary statistics\n        environment_results = [r for r in test_results.values() if isinstance(r, dict) and 'overall_success' in r]\n        \n        if environment_results:\n            successful_environments = sum(1 for r in environment_results if r.get('overall_success', False))\n            enhanced_report['summary']['overall_success_rate'] = successful_environments / len(environment_results)\n            enhanced_report['summary']['total_test_duration'] = sum(r.get('total_duration_seconds', 0) for r in environment_results)\n        \n        # Save report\n        with open(output_path, 'w') as f:\n            json.dump(enhanced_report, f, indent=2)\n        \n        logger.info(f\"üìä Deployment test report saved to {output_path}\")\n        return str(output_path)\n\ndef main():\n    \"\"\"Main function for command-line usage.\"\"\"\n    parser = argparse.ArgumentParser(description=\"Deployment Environment Tester\")\n    parser.add_argument(\"--environment\", \n                       choices=['development', 'staging', 'production', 'all'],\n                       default='development',\n                       help=\"Environment to test\")\n    parser.add_argument(\"--setup-only\", action=\"store_true\", help=\"Only setup environment\")\n    parser.add_argument(\"--cleanup-only\", action=\"store_true\", help=\"Only cleanup environment\")\n    parser.add_argument(\"--test-api\", action=\"store_true\", help=\"Test API endpoints only\")\n    parser.add_argument(\"--test-classification\", action=\"store_true\", help=\"Test classification only\")\n    parser.add_argument(\"--test-performance\", action=\"store_true\", help=\"Test performance only\")\n    parser.add_argument(\"--output\", help=\"Output path for test report\")\n    \n    args = parser.parse_args()\n    \n    # Initialize tester\n    tester = DeploymentEnvironmentTester()\n    \n    try:\n        if args.cleanup_only:\n            if args.environment == 'all':\n                for env in ['development', 'staging', 'production']:\n                    tester.cleanup_environment(env)\n            else:\n                tester.cleanup_environment(args.environment)\n            return\n        \n        if args.setup_only:\n            success = tester.setup_environment(args.environment)\n            sys.exit(0 if success else 1)\n        \n        # Run tests\n        test_results = {}\n        \n        if args.environment == 'all':\n            environments = ['development', 'staging', 'production']\n        else:\n            environments = [args.environment]\n        \n        for env in environments:\n            if args.test_api:\n                test_results[env] = tester.test_spatial_api_endpoints(env)\n            elif args.test_classification:\n                test_results[env] = tester.test_spatial_classification(env)\n            elif args.test_performance:\n                test_results[env] = tester.test_performance_metrics(env)\n            else:\n                # Run comprehensive test\n                test_results[env] = tester.run_comprehensive_test(env)\n        \n        # Generate report\n        report_path = tester.generate_deployment_report(test_results, args.output)\n        \n        # Determine overall success\n        overall_success = all(result.get('overall_success', False) \n                            for result in test_results.values() \n                            if isinstance(result, dict))\n        \n        logger.info(f\"\\nüéØ Deployment testing completed: {'‚úÖ ALL PASSED' if overall_success else '‚ùå SOME FAILED'}\")\n        logger.info(f\"üìä Report: {report_path}\")\n        \n        sys.exit(0 if overall_success else 1)\n    \n    except Exception as e:\n        logger.error(f\"Deployment testing failed: {str(e)}\")\n        sys.exit(1)\n\nif __name__ == \"__main__\":\n    main()
