#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Strukturbasiertes Pruning für MicroPizzaNetV2

Dieses Skript implementiert strukturbasiertes Pruning (Entfernen ganzer Filter/Kanäle) 
für das MicroPizzaNetV2-Modell. Es analysiert die Wichtigkeit der Filter anhand ihrer
L1-Norm und entfernt die Filter mit den niedrigsten Werten.

Funktionsweise:
1. Lädt das vortrainierte MicroPizzaNetV2-Modell
2. Analysiert die Filter und identifiziert unwichtige Kanäle basierend auf L1-Norm
3. Erstellt ein neues, gepruntes Modell mit reduzierter Kanalanzahl
4. Trainiert das geprunte Modell kurz nach (optional)
5. Quantisiert das geprunte Modell (optional)
6. Speichert das geprunte Modell und erstellt einen Bericht

Verwendung:
    python pruning_tool.py --sparsity 0.3 --fine_tune --quantize
"""

import os
import sys
import json
import time
import torch
import numpy as np
import argparse
import logging
from datetime import datetime
from pathlib import Path
import torch.nn as nn
import torch.optim as optim
import torchvision.transforms as transforms

# Füge das Projekt-Root zum Pythonpfad hinzu, um Imports zu ermöglichen
project_root = Path(__file__).parent.parent
sys.path.append(str(project_root))

# Importiere die benötigten Module aus dem Projekt
from src.pizza_detector import MicroPizzaNetV2, InvertedResidualBlock
from torchvision.datasets import ImageFolder
from torch.utils.data import DataLoader, random_split

# Logging konfigurieren
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(os.path.join(os.path.dirname(os.path.dirname(__file__)), 'pruning_clustering.log')),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger('pruning_tool')

def parse_arguments():
    """Kommandozeilenargumente parsen"""
    parser = argparse.ArgumentParser(description='Strukturbasiertes Pruning für MicroPizzaNetV2')
    parser.add_argument('--sparsity', type=float, default=0.3,
                        help='Ziel-Sparsity: Anteil der zu entfernenden Filter (0.0-0.9)')
    parser.add_argument('--model_path', type=str, default=None,
                        help='Pfad zum vortrainierten Modell (.pth)')
    parser.add_argument('--fine_tune', action='store_true',
                        help='Gepruntes Modell nach dem Pruning feintunen')
    parser.add_argument('--fine_tune_epochs', type=int, default=5,
                        help='Anzahl der Epochen für Finetuning')
    parser.add_argument('--batch_size', type=int, default=32,
                        help='Batch-Größe für Training/Evaluierung')
    parser.add_argument('--quantize', action='store_true',
                        help='Gepruntes Modell zu Int8 quantisieren')
    parser.add_argument('--output_dir', type=str, default='models',
                        help='Ausgabeverzeichnis für geprunte Modelle')
    
    return parser.parse_args()

def get_model(model_path=None):
    """
    Lädt das vortrainierte MicroPizzaNetV2-Modell
    Wenn kein spezifischer Pfad angegeben ist, wird ein Modell trainiert
    
    Args:
        model_path (str, optional): Pfad zum vortrainierten Modell. Falls None,
                                   wird args.model_path verwendet oder ein neues 
                                   Modell trainiert.
    """
    global args
    model = MicroPizzaNetV2(num_classes=4)
    
    # Verwende den expliziten model_path, wenn angegeben
    path = model_path if model_path is not None else (args.model_path if 'args' in globals() else None)
    
    if path:
        if os.path.exists(path):
            logger.info(f"Lade vortrainiertes Modell von {path}")
            model.load_state_dict(torch.load(path))
        else:
            logger.warning(f"Modellpfad {path} nicht gefunden. Verwende untrainiertes Modell.")
    else:
        logger.info("Kein Modellpfad angegeben. Trainiere ein neues Modell.")
        train_quick_model(model)
    
    return model

def create_dataloaders(batch_size=32, img_size=48):
    """
    Erstellt DataLoader für Trainings- und Validierungsdaten
    
    Args:
        batch_size (int): Batch-Größe für DataLoader
        img_size (int): Zielgröße für Bilder
        
    Returns:
        train_loader, val_loader: DataLoader für Training und Validierung
    """
    # Definiere Transformationen
    train_transform = transforms.Compose([
        transforms.Resize((img_size, img_size)),
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation(10),
        transforms.ColorJitter(brightness=0.1, contrast=0.1),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])
    
    val_transform = transforms.Compose([
        transforms.Resize((img_size, img_size)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])
    
    # Pfade zu den Datenverzeichnissen
    data_dir = project_root / 'augmented_pizza'
    
    # Versuche, die verfügbaren Unterverzeichnisse zu verwenden
    categories = [d for d in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, d)) and not d.startswith('.')]
    logger.info(f"Gefundene Kategorien: {categories}")
    
    if not categories:
        # Fallback: Verwende die Legacy-Daten
        data_dir = project_root / 'augmented_pizza_legacy'
        
    # Erstelle Dataset mit ImageFolder
    full_dataset = ImageFolder(
        root=data_dir,
        transform=train_transform
    )
    
    # Teile Dataset in Training und Validierung auf
    dataset_size = len(full_dataset)
    train_size = int(dataset_size * 0.8)
    val_size = dataset_size - train_size
    
    train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])
    
    # Überschreibe Transformation für Validierungsdaten
    val_dataset.dataset.transform = val_transform
    
    # Erstelle DataLoader
    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=4,
        pin_memory=True
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=4,
        pin_memory=True
    )
    
    logger.info(f"Datensatz geladen: {dataset_size} Bilder, {train_size} für Training, {val_size} für Validierung")
    
    return train_loader, val_loader

def train_quick_model(model, epochs=10, batch_size=32):
    """
    Trainiert ein Modell für einige Epochen, falls kein vortrainiertes Modell vorhanden ist
    
    Args:
        model: Das zu trainierende Modell
        epochs: Anzahl der Trainings-Epochen
        batch_size: Batch-Größe für das Training
    """
    # Datensatz laden
    train_loader, val_loader = create_dataloaders(batch_size=batch_size)
    
    # Verlustfunktion und Optimizer definieren
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=0.001)
    
    # Gerät festlegen (CPU/GPU)
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    model.to(device)
    
    # Training
    logger.info(f"Trainiere neues Modell für {epochs} Epochen...")
    for epoch in range(epochs):
        model.train()
        running_loss = 0.0
        correct = 0
        total = 0
        
        for inputs, labels in train_loader:
            inputs, labels = inputs.to(device), labels.to(device)
            
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            
            running_loss += loss.item()
            _, predicted = outputs.max(1)
            total += labels.size(0)
            correct += predicted.eq(labels).sum().item()
        
        # Ausgabe nach jeder Epoche
        train_loss = running_loss / len(train_loader)
        train_acc = 100. * correct / total
        logger.info(f'Epoche {epoch+1}/{epochs}, Verlust: {train_loss:.4f}, Genauigkeit: {train_acc:.2f}%')
    
    # Speichere das trainierte Modell
    output_dir = Path(args.output_dir)
    output_dir.mkdir(exist_ok=True, parents=True)
    model_save_path = output_dir / 'micropizzanetv2_base.pth'
    torch.save(model.state_dict(), model_save_path)
    logger.info(f"Basis-Modell gespeichert unter {model_save_path}")
    
    # Evaluiere das trainierte Modell
    evaluate_model(model, val_loader, device)
    
    return model

def evaluate_model(model, data_loader, device):
    """
    Evaluiert ein Modell auf dem Validierungsdatensatz
    """
    model.eval()
    correct = 0
    total = 0
    
    with torch.no_grad():
        for inputs, labels in data_loader:
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            _, predicted = outputs.max(1)
            total += labels.size(0)
            correct += predicted.eq(labels).sum().item()
    
    accuracy = 100. * correct / total
    logger.info(f'Validierungsgenauigkeit: {accuracy:.2f}%')
    return accuracy

def get_filter_importance(model):
    """
    Berechnet die Wichtigkeit jedes Filters basierend auf L1-Norm
    """
    importance_dict = {}
    
    # Für jede Komponente des Modells
    for name, module in model.named_modules():
        # Filter für Convolutional Layer
        if isinstance(module, nn.Conv2d) and module.groups == 1:  # Normale Faltung (keine Depthwise)
            # L1-Norm jedes Filters berechnen
            weight = module.weight.data.clone()
            importance = torch.norm(weight.view(weight.size(0), -1), p=1, dim=1)
            importance_dict[name] = importance
    
    return importance_dict

def create_pruned_model(model, sparsity):
    """
    Erstellt eine geprunte Version des MicroPizzaNetV2-Modells
    
    - Für Convolutional Layers: Entfernt Filter mit niedriger Wichtigkeit
    - Passt nachfolgende Layer entsprechend an
    
    Args:
        model: Das zu prunende Modell
        sparsity: Die Sparsity-Rate (0.0 bis 1.0)
        
    Returns:
        Ein gepruntes MicroPizzaNetV2-Modell mit reduzierten Kanälen
    """
    logger.info(f"Erstelle gepruntes Modell mit Sparsity {sparsity:.2f}")
    
    # 1. Berechne Wichtigkeit der Filter (L1-Norm) für alle Conv-Layer
    importance_dict = {}
    
    for name, module in model.named_modules():
        if isinstance(module, nn.Conv2d) and module.groups == 1:  # Überspringe Depthwise Conv-Layer
            # Berechne L1-Norm der Filter (über alle Ausgabe-Kanäle)
            weight = module.weight.data.clone()
            importance = torch.norm(weight.view(weight.size(0), -1), p=1, dim=1)
            importance_dict[name] = importance
            logger.info(f"Layer {name}: {len(importance)} Filter, Wichtigkeitsbereich: {importance.min().item():.6f} - {importance.max().item():.6f}")
    
    # 2. Bestimme, welche Filter entfernt werden sollen
    prune_targets = {}
    
    # Für jede Komponente des Modells
    for name, module in model.named_modules():
        if isinstance(module, nn.Conv2d) and name in importance_dict:
            # Sortiere Filter nach Wichtigkeit
            filter_importance = importance_dict[name]
            n_filters = len(filter_importance)
            n_prune = int(n_filters * sparsity)
            
            # Indizes der unwichtigsten Filter
            _, indices = torch.sort(filter_importance)
            prune_indices = indices[:n_prune].tolist()
            keep_indices = indices[n_prune:].tolist()
            
            prune_targets[name] = {
                'prune_indices': prune_indices,
                'keep_indices': keep_indices
            }
            logger.info(f"Layer {name}: behalte {len(keep_indices)}/{n_filters} Filter, entferne {len(prune_indices)}")
    
    # 3. Erstelle ein neues Modell mit reduzierter Kanalanzahl
    pruned_model = MicroPizzaNetV2(num_classes=4)
    
    # 4. Kopiere die Gewichte der wichtigen Filter
    with torch.no_grad():
        # Block 1: Regulärer Conv2d
        if 'block1.0' in prune_targets:
            # Kopiere die beibehaltenen Filter vom ersten Conv-Layer
            keep_indices = prune_targets['block1.0']['keep_indices']
            pruned_model.block1[0].weight.data = model.block1[0].weight.data[keep_indices].clone()
            
            # Aktualisiere BatchNorm-Layer nach dem Pruning
            pruned_model.block1[1].weight.data = model.block1[1].weight.data[keep_indices].clone()
            pruned_model.block1[1].bias.data = model.block1[1].bias.data[keep_indices].clone()
            pruned_model.block1[1].running_mean.data = model.block1[1].running_mean.data[keep_indices].clone()
            pruned_model.block1[1].running_var.data = model.block1[1].running_var.data[keep_indices].clone()
        
        # Block 2: InvertedResidualBlock
        # Struktur:
        # 0: Pointwise Conv (Expansion)
        # 1: BatchNorm
        # 2: ReLU6
        # 3: Depthwise Conv
        # 4: BatchNorm
        # 5: ReLU6
        # 6: Pointwise Conv (Projection)
        # 7: BatchNorm
        
        # 1. Expansion Layer (1x1 Pointwise)
        if 'block2.conv.0' in prune_targets:
            # Eingabekanäle basieren auf beibehaltenen Filtern von block1.0
            src_block1_keep = prune_targets['block1.0']['keep_indices'] if 'block1.0' in prune_targets else list(range(model.block1[0].weight.size(0)))
            src_block2_keep = prune_targets['block2.conv.0']['keep_indices']
            
            # Aktualisiere den ersten Conv-Layer im Block2 (Expansion Layer)
            pruned_weights = model.block2.conv[0].weight.data[src_block2_keep][:, src_block1_keep].clone()
            pruned_model.block2.conv[0].weight.data = pruned_weights
            
            # Aktualisiere BatchNorm nach der ersten Convolution
            pruned_model.block2.conv[1].weight.data = model.block2.conv[1].weight.data[src_block2_keep].clone()
            pruned_model.block2.conv[1].bias.data = model.block2.conv[1].bias.data[src_block2_keep].clone()
            pruned_model.block2.conv[1].running_mean.data = model.block2.conv[1].running_mean.data[src_block2_keep].clone()
            pruned_model.block2.conv[1].running_var.data = model.block2.conv[1].running_var.data[src_block2_keep].clone()
        
        # 2. Depthwise Conv-Layer (groups = in_channels)
        if 'block2.conv.3' in prune_targets:
            src_depthwise_keep = prune_targets['block2.conv.0']['keep_indices'] if 'block2.conv.0' in prune_targets else list(range(model.block2.conv[0].weight.size(0)))
            
            # Aktualisiere den Depthwise Conv-Layer in Block2
            # Bei Depthwise Convolution: in_channels = out_channels = groups
            pruned_weights = model.block2.conv[3].weight.data[src_depthwise_keep].clone()
            pruned_model.block2.conv[3].weight.data = pruned_weights
            pruned_model.block2.conv[3].groups = len(src_depthwise_keep)
            
            # Aktualisiere BatchNorm nach dem Depthwise Conv
            pruned_model.block2.conv[4].weight.data = model.block2.conv[4].weight.data[src_depthwise_keep].clone()
            pruned_model.block2.conv[4].bias.data = model.block2.conv[4].bias.data[src_depthwise_keep].clone()
            pruned_model.block2.conv[4].running_mean.data = model.block2.conv[4].running_mean.data[src_depthwise_keep].clone()
            pruned_model.block2.conv[4].running_var.data = model.block2.conv[4].running_var.data[src_depthwise_keep].clone()
        
        # 3. Pointwise Conv-Layer (1x1) - Projection Layer
        if 'block2.conv.6' in prune_targets:
            src_pointwise_in_keep = prune_targets['block2.conv.0']['keep_indices'] if 'block2.conv.0' in prune_targets else list(range(model.block2.conv[0].weight.size(0)))
            src_pointwise_out_keep = prune_targets['block2.conv.6']['keep_indices']
            
            # Aktualisiere den Pointwise Conv-Layer in Block2 (Projection Layer)
            pruned_weights = model.block2.conv[6].weight.data[src_pointwise_out_keep][:, src_pointwise_in_keep].clone()
            pruned_model.block2.conv[6].weight.data = pruned_weights
            
            # Aktualisiere BatchNorm nach dem Pointwise Conv
            pruned_model.block2.conv[7].weight.data = model.block2.conv[7].weight.data[src_pointwise_out_keep].clone()
            pruned_model.block2.conv[7].bias.data = model.block2.conv[7].bias.data[src_pointwise_out_keep].clone()
            pruned_model.block2.conv[7].running_mean.data = model.block2.conv[7].running_mean.data[src_pointwise_out_keep].clone()
            pruned_model.block2.conv[7].running_var.data = model.block2.conv[7].running_var.data[src_pointwise_out_keep].clone()
        
        # 4. Fully Connected Layer anpassen
        # Übertrage die Gewichte vom classifier
        # Input sind die Ausgabekanäle vom Block 2
        if 'block2.conv.6' in prune_targets:
            out_channels_keep = prune_targets['block2.conv.6']['keep_indices']
            pruned_model.classifier[2].weight.data = model.classifier[2].weight.data[:, out_channels_keep].clone()
            pruned_model.classifier[2].bias.data = model.classifier[2].bias.data.clone()
    
    logger.info(f"Geprunte Modellstruktur:\n{pruned_model}")
    logger.info(f"Original Parameter: {model.count_parameters():,}")
    logger.info(f"Geprunte Parameter: {pruned_model.count_parameters():,}")
    logger.info(f"Reduktion: {100 * (1 - pruned_model.count_parameters() / model.count_parameters()):.2f}%")
    
    return pruned_model
            pruned_model.block2.conv[1].bias.data = model.block2.conv[1].bias.data[src_block2_keep].clone()
            pruned_model.block2.conv[1].running_mean.data = model.block2.conv[1].running_mean.data[src_block2_keep].clone()
            pruned_model.block2.conv[1].running_var.data = model.block2.conv[1].running_var.data[src_block2_keep].clone()
        
        # Depthwise Conv-Layer (groups = in_channels)
        if 'block2.conv.3' in prune_targets:
            src_depthwise_keep = prune_targets['block2.conv.3']['keep_indices']
            
            # Aktualisiere den Depthwise Conv-Layer in Block2
            # Bei Depthwise Convolution: in_channels = out_channels = groups
            pruned_weights = model.block2.conv[3].weight.data[src_depthwise_keep].clone()
            pruned_model.block2.conv[3].weight.data = pruned_weights
            
            # Aktualisiere BatchNorm nach dem Depthwise Conv
            pruned_model.block2.conv[4].weight.data = model.block2.conv[4].weight.data[src_depthwise_keep].clone()
            pruned_model.block2.conv[4].bias.data = model.block2.conv[4].bias.data[src_depthwise_keep].clone()
            pruned_model.block2.conv[4].running_mean.data = model.block2.conv[4].running_mean.data[src_depthwise_keep].clone()
            pruned_model.block2.conv[4].running_var.data = model.block2.conv[4].running_var.data[src_depthwise_keep].clone()
        
        # Pointwise Conv-Layer (1x1)
        if 'block2.conv.6' in prune_targets:
            src_pointwise_in_keep = prune_targets['block2.conv.3']['keep_indices'] if 'block2.conv.3' in prune_targets else list(range(model.block2.conv[3].weight.size(0)))
            src_pointwise_out_keep = prune_targets['block2.conv.6']['keep_indices']
            
            # Aktualisiere den Pointwise Conv-Layer in Block2 (Projection Layer)
            pruned_weights = model.block2.conv[6].weight.data[src_pointwise_out_keep][:, src_pointwise_in_keep].clone()
            pruned_model.block2.conv[6].weight.data = pruned_weights
            
            # Aktualisiere BatchNorm nach dem Pointwise Conv
            pruned_model.block2.conv[7].weight.data = model.block2.conv[7].weight.data[src_pointwise_out_keep].clone()
            pruned_model.block2.conv[7].bias.data = model.block2.conv[7].bias.data[src_pointwise_out_keep].clone()
            pruned_model.block2.conv[7].running_mean.data = model.block2.conv[7].running_mean.data[src_pointwise_out_keep].clone()
            pruned_model.block2.conv[7].running_var.data = model.block2.conv[7].running_var.data[src_pointwise_out_keep].clone()
        
        # Fully Connected Layer anpassen
        # Übertrage die Gewichte vom classifier
        # Input sind die Ausgabekanäle vom Block 2
        if 'block2.conv.6' in prune_targets:
            out_channels_keep = prune_targets['block2.conv.6']['keep_indices']
            pruned_model.classifier[2].weight.data = model.classifier[2].weight.data[:, out_channels_keep].clone()
            pruned_model.classifier[2].bias.data = model.classifier[2].bias.data.clone()
    
    logger.info(f"Geprunte Modellstruktur:\n{pruned_model}")
    logger.info(f"Original Parameter: {model.count_parameters():,}")
    logger.info(f"Geprunte Parameter: {pruned_model.count_parameters():,}")
    logger.info(f"Reduktion: {100 * (1 - pruned_model.count_parameters() / model.count_parameters()):.2f}%")
    
    return pruned_model

def quantize_model(model, train_loader=None, input_size=(3, 48, 48)):
    """
    Quantisiert das Modell zu Int8 (für TensorFlow Lite)
    
    Args:
        model: Das zu quantisierende PyTorch-Modell
        train_loader: DataLoader mit Trainingsdaten für die Kalibrierung
        input_size: Tupel mit der Eingabegröße (channels, height, width)
    
    Returns:
        Das quantisierte Modell
    """
    logger.info("Quantisiere Modell zu Int8 (TensorFlow Lite-Format)")
    
    # Importiere das quantization-Modul
    try:
        import torch
        from torch import quantization
        
        # Create a copy of the model to avoid modifying the original
        model_copy = type(model)(num_classes=model.classifier[2].out_features)
        model_copy.load_state_dict(model.state_dict())
        model_to_quantize = model_copy
        
        # Model in Eval-Modus setzen
        model_to_quantize.eval()
        
        # Konfiguriere die Modell-Quantisierung
        # Für x86/x64: 'fbgemm'
        # Für ARM/mobile: 'qnnpack'
        backend = 'qnnpack'
        model_to_quantize.qconfig = quantization.get_default_qconfig(backend)
        
        # Füge Observer-Module hinzu
        model_prepared = quantization.prepare(model_to_quantize)
        
        # Kalibrierung durchführen
        if train_loader is not None:
            logger.info(f"Kalibriere Quantisierung mit {min(10, len(train_loader))} Batches echter Trainingsdaten...")
            # Verwende eine Teilmenge der Trainingsdaten zur Kalibrierung
            num_batches = min(10, len(train_loader))
            
            with torch.no_grad():
                for i, (inputs, _) in enumerate(train_loader):
                    if i >= num_batches:
                        break
                    model_prepared(inputs)
                    
                    # Fortschrittsanzeige
                    if (i + 1) % 2 == 0 or (i + 1) == num_batches:
                        logger.info(f"Kalibrierung: Batch {i+1}/{num_batches}")
        else:
            # Erstelle künstliche Kalibrierungsdaten
            logger.info("Kalibriere Quantisierung mit künstlichen Daten...")
            # Erzeuge verschiedene Testmuster für bessere Kalibrierung
            with torch.no_grad():
                # Zufallsdaten
                for _ in range(10):
                    dummy_input = torch.randn(1, *input_size)
                    model_prepared(dummy_input)
                
                # Konstante Daten (unterschiedliche Werte)
                values = [0.0, 0.25, 0.5, 0.75, 1.0]
                for val in values:
                    dummy_input = torch.ones(1, *input_size) * val
                    model_prepared(dummy_input)
        
        # Konvertiere zu einem statisch quantisierten Modell
        quantized_model = quantization.convert(model_prepared)
        
        # Vergleiche Parameteranzahl und Modelgröße
        original_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        quantized_params = sum(p.numel() for p in quantized_model.parameters() if p.requires_grad)
        
        # Berechne ungefähre Modellgröße in KB (vereinfacht)
        original_size_kb = original_params * 4 / 1024  # Float32 = 4 Bytes
        quantized_size_kb = quantized_params / 1024    # Int8 = 1 Byte
        
        logger.info(f"Originale Parameter: {original_params:,} (ca. {original_size_kb:.2f} KB)")
        logger.info(f"Quantisierte Parameter: {quantized_params:,} (ca. {quantized_size_kb:.2f} KB)")
        logger.info(f"Größenreduktion: {100 * (1 - quantized_size_kb / original_size_kb):.2f}%")
        logger.info("Quantisierung erfolgreich abgeschlossen")
        
        return quantized_model
        
    except Exception as e:
        logger.error(f"Fehler bei der Quantisierung: {e}")
        logger.warning("Quantisierung fehlgeschlagen. Verwende unquantisiertes Modell.")
        import traceback
        traceback.print_exc()
        return model
        model_copy = type(model)(num_classes=model.classifier[2].out_features)
        model_copy.load_state_dict(model.state_dict())
        model_to_quantize = model_copy
        
        # Model in Eval-Modus setzen
        model_to_quantize.eval()
        
        # Konfiguriere die Modell-Quantisierung
        # Für x86/x64: 'fbgemm'
        # Für ARM/mobile: 'qnnpack'
        backend = 'qnnpack'
        model_to_quantize.qconfig = quantization.get_default_qconfig(backend)
        
        # Füge Observer-Module hinzu
        model_prepared = quantization.prepare(model_to_quantize)
        
        # Kalibrierung durchführen
        if train_loader is not None:
            logger.info(f"Kalibriere Quantisierung mit {min(10, len(train_loader))} Batches echter Trainingsdaten...")
            # Verwende eine Teilmenge der Trainingsdaten zur Kalibrierung
            num_batches = min(10, len(train_loader))
            
            with torch.no_grad():
                for i, (inputs, _) in enumerate(train_loader):
                    if i >= num_batches:
                        break
                    model_prepared(inputs)
                    
                    # Fortschrittsanzeige
                    if (i + 1) % 2 == 0 or (i + 1) == num_batches:
                        logger.info(f"Kalibrierung: Batch {i+1}/{num_batches}")
        else:
            # Erstelle künstliche Kalibrierungsdaten
            logger.info("Kalibriere Quantisierung mit künstlichen Daten...")
            # Erzeuge verschiedene Testmuster für bessere Kalibrierung
            with torch.no_grad():
                # Zufallsdaten
                for _ in range(10):
                    dummy_input = torch.randn(1, *input_size)
                    model_prepared(dummy_input)
                
                # Konstante Daten (unterschiedliche Werte)
                values = [0.0, 0.25, 0.5, 0.75, 1.0]
                for val in values:
                    dummy_input = torch.ones(1, *input_size) * val
                    model_prepared(dummy_input)
        
        # Konvertiere zu einem statisch quantisierten Modell
        quantized_model = quantization.convert(model_prepared)
        
        # Vergleiche Parameteranzahl und Modelgröße
        original_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        quantized_params = sum(p.numel() for p in quantized_model.parameters() if p.requires_grad)
        
        # Berechne ungefähre Modellgröße in KB (vereinfacht)
        original_size_kb = original_params * 4 / 1024  # Float32 = 4 Bytes
        quantized_size_kb = quantized_params / 1024    # Int8 = 1 Byte
        
        logger.info(f"Originale Parameter: {original_params:,} (ca. {original_size_kb:.2f} KB)")
        logger.info(f"Quantisierte Parameter: {quantized_params:,} (ca. {quantized_size_kb:.2f} KB)")
        logger.info(f"Größenreduktion: {100 * (1 - quantized_size_kb / original_size_kb):.2f}%")
        logger.info("Quantisierung erfolgreich abgeschlossen")
        
        return quantized_model
        
    except Exception as e:
        logger.error(f"Fehler bei der Quantisierung: {e}")
        logger.warning("Quantisierung fehlgeschlagen. Verwende unquantisiertes Modell.")
        import traceback
        traceback.print_exc()
        return model

def save_pruned_model(model, sparsity, output_dir="models", quantized=False, include_metrics=True):
    """
    Speichert das geprunte Modell und erstellt einen Bericht
    
    Args:
        model: Das zu speichernde Modell
        sparsity: Die angewendete Sparsity-Rate (0.0 bis 1.0)
        output_dir: Ausgabeverzeichnis
        quantized: Ob das Modell quantisiert wurde
        include_metrics: Ob Modellmetriken im Bericht enthalten sein sollen
    
    Returns:
        dict: Dictionary mit Berichtsdaten und Pfad zum gespeicherten Modell
    """
    output_dir = Path(output_dir)
    output_dir.mkdir(exist_ok=True, parents=True)
    
    # Modelltyp und Name bestimmen
    model_type = "quantized" if quantized else "pruned"
    sparsity_str = f"{int(sparsity*100)}" if sparsity > 0 else "base"
    model_name = f"micropizzanetv2_{model_type}_s{sparsity_str}"
    torch_path = output_dir / f"{model_name}.pth"
    
    # Modell speichern
    logger.info(f"Speichere {model_type} Modell nach: {torch_path}")
    torch.save(model.state_dict(), torch_path)
    
    # Bericht erstellen
    report = {
        "model_name": model_name,
        "model_type": model_type,
        "sparsity": sparsity,
        "model_path": str(torch_path)
    }
    
    if include_metrics:
        # Sammle Modellmetriken
        num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        model_size_kb = sum(p.nelement() * p.element_size() for p in model.parameters()) / 1024
        
        report.update({
            "parameters": num_params,
            "model_size_kb": model_size_kb,
            "saved_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        })
        
        # Speichere Bericht als JSON
        report_path = output_dir / f"{model_name}_report.json"
        with open(report_path, 'w') as f:
            json.dump(report, f, indent=2)
        
        logger.info(f"Modellbericht gespeichert unter: {report_path}")
    
    return report
    
    # Für TFLite-Export würden wir hier weitere Schritte durchführen
    # Dies ist ein Platzhalter für die tatsächliche Implementierung
    tflite_path = output_dir / f"{model_name}.tflite"
    
    # Erstelle Bericht
    report = {
        "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "model_name": model_name,
        "sparsity": sparsity,
        "quantized": quantized,
        "parameters": model.count_parameters(),
        "model_paths": {
            "pytorch": str(torch_path),
            "tflite": str(tflite_path)
        }
    }
    
    # Speichere Bericht
    report_dir = Path("output/model_optimization")
    report_dir.mkdir(exist_ok=True, parents=True)
    report_path = report_dir / f"pruning_report_s{int(sparsity*100)}.json"
    
    with open(report_path, 'w') as f:
        json.dump(report, f, indent=2)
    
    logger.info(f"Gepruntes Modell gespeichert: {torch_path}")
    logger.info(f"Pruning-Bericht erstellt: {report_path}")
    
    return report

def main(custom_args=None):
    """
    Hauptfunktion
    
    Args:
        custom_args: Liste mit benutzerdefinierten Befehlszeilenargumenten
                    oder None, um sys.argv zu verwenden
    """
    global args
    if custom_args is not None:
        # Wenn benutzerdefinierte Argumente übergeben wurden, parse diese
        orig_argv = sys.argv
        sys.argv = ['pruning_tool.py'] + custom_args
        args = parse_arguments()
        sys.argv = orig_argv
    else:
        # Sonst parse die Kommandozeilenargumente wie gewohnt
        args = parse_arguments()
    
    start_time = time.time()
    logger.info(f"Starte strukturbasiertes Pruning mit Ziel-Sparsity {args.sparsity:.2f}")
    
    # Lade oder trainiere das Modell
    model = get_model()
    logger.info(f"MicroPizzaNetV2 geladen. Parameter: {model.count_parameters():,}")
    
    # Berechne Filter-Wichtigkeit
    importance_dict = get_filter_importance(model)
    
    # Erstelle gepruntes Modell
    pruned_model = create_pruned_model(model, importance_dict, args.sparsity)
    logger.info(f"Gepruntes Modell erstellt. Parameter: {pruned_model.count_parameters():,}")
    
    # Optional: Feintuning
    if args.fine_tune:
        logger.info(f"Starte Feintuning für {args.fine_tune_epochs} Epochen")
        train_loader, val_loader = create_dataloaders(batch_size=args.batch_size)
        device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
        pruned_model.to(device)
        
        # Setup für Feintuning
        criterion = nn.CrossEntropyLoss()
        optimizer = optim.Adam(pruned_model.parameters(), lr=0.0005)
        
        # Feintuning-Schleife
        for epoch in range(args.fine_tune_epochs):
            pruned_model.train()
            running_loss = 0.0
            
            for inputs, labels in train_loader:
                inputs, labels = inputs.to(device), labels.to(device)
                
                optimizer.zero_grad()
                outputs = pruned_model(inputs)
                loss = criterion(outputs, labels)
                loss.backward()
                optimizer.step()
                
                running_loss += loss.item()
            
            epoch_loss = running_loss / len(train_loader)
            logger.info(f'Feintuning Epoche {epoch+1}/{args.fine_tune_epochs}, Verlust: {epoch_loss:.4f}')
            
            # Evaluiere nach jeder Epoche
            accuracy = evaluate_model(pruned_model, val_loader, device)
    
    # Optional: Quantisierung
    final_model = pruned_model
    if args.quantize:
        train_loader, _ = create_dataloaders(batch_size=args.batch_size)
        final_model = quantize_model(pruned_model, train_loader)
    
    # Speichere Modell und Bericht
    report = save_pruned_model(final_model, args.sparsity, output_dir=args.output_dir, quantized=args.quantize)
    
    # Ausgabe
    elapsed_time = time.time() - start_time
    logger.info(f"Pruning abgeschlossen in {elapsed_time:.2f} Sekunden")
    logger.info(f"Originale Parameter: {model.count_parameters():,}")
    logger.info(f"Geprunte Parameter: {final_model.count_parameters():,}")
    logger.info(f"Reduktion: {100 * (1 - final_model.count_parameters() / model.count_parameters()):.2f}%")
    
    return {
        'original_model': model,
        'pruned_model': final_model,
        'report': report,
        'sparsity': args.sparsity
    }

if __name__ == "__main__":
    main()
